# 1¬∫ Hackathon em Controle Social: Desafio Participa DF
## Categoria: Acesso √† Informa√ß√£o

### Nome do Projeto
**Classifica√ß√£o Autom√°tica de Solicita√ß√µes com Dados Pessoais (PII)**

### Descri√ß√£o do Projeto
Este projeto tem como objetivo identificar automaticamente se uma solicita√ß√£o de acesso √† informa√ß√£o cont√©m **dados pessoais sens√≠veis**, auxiliando √≥rg√£os p√∫blicos no tratamento adequado das demandas conforme a Lei Geral de Prote√ß√£o de Dados (LGPD).

A solu√ß√£o utiliza um modelo de **Processamento de Linguagem Natural (PLN)** treinado para classificar textos extra√≠dos de solicita√ß√µes do Participa DF, indicando se h√° ou n√£o presen√ßa de dados pessoais. O foco √© apoiar a transpar√™ncia p√∫blica com responsabilidade e seguran√ßa da informa√ß√£o.

---

## Tecnologias Utilizadas
- Python 3.9+
- Pandas
- NumPy
- Scikit-learn
- Transformers (Hugging Face)
- PyTorch
- Jupyter Notebook

---

## Autores
- **Autora:** Amanda dos Santos Pereira  
- **Coautora:** Suzany de Almeida Lima  

---

# 1. Instru√ß√µes de Instala√ß√£o e Depend√™ncias

## 1.1 Pr√©-requisitos
Antes de iniciar, certifique-se de ter instalado:

- Python **3.9 ou superior**
- Git
- Ambiente Windows (testado em Windows 10+)
- Acesso √† internet para download do modelo

> ‚ö†Ô∏è **Observa√ß√£o:** O projeto **n√£o √© compat√≠vel com Python 4.x**, pois essa vers√£o ainda n√£o √© est√°vel e n√£o √© suportada pelas bibliotecas utilizadas.

---

## 1.2 Gerenciamento de Pacotes
O projeto utiliza um arquivo `requirements.txt`, que cont√©m todas as depend√™ncias necess√°rias para execu√ß√£o, permitindo a instala√ß√£o automatizada do ambiente.

Exemplo de depend√™ncias inclu√≠das:
```txt
pandas
numpy
torch
transformers
scikit-learn
```

## 1.3 Cria√ß√£o e Configura√ß√£o do Ambiente

#### 1.3.1 Abra um terminal na pasta raiz do projeto.
Crie um ambiente virtual:
`python -m venv .venv`

#### 1.3.2 Ative o ambiente virtual:
`.venv\Scripts\Activate.ps1`


#### 1.3.3 Instale as depend√™ncias:
`pip install -r requirements.txt`

## 2. Cria√ß√£o do Dataset
O dataset foi constru√≠do a partir de solicita√ß√µes reais disponibilizadas pelo `portaldatransparencia.gov.br` em Busca de Pedidos e Respostas dos anos de 2025 e 2024.

Etapas realizadas:
- Limpeza de textos
- Normaliza√ß√£o (min√∫sculas, remo√ß√£o de caracteres especiais)
- Anonimiza√ß√£o
- Rotulagem supervisionada (possui_dados_pessoais = True/False)
- Valida√ß√£o dos r√≥tulos
- Esses processos garantem consist√™ncia e confiabilidade para o treinamento do modelo.
- As etapas de cria√ß√£o do dataset est√£o documentadas no diret√≥rio:
`pipelines/dataset_generation`


## 3 Treinamento e Publica√ß√£o do Modelo

### O treinamento do modelo foi realizado no notebook:
`pipelines/training/model_training.ipynb`

#### Etapas do treinamento:

- Tokeniza√ß√£o dos textos

- Fine-tuning do modelo DistilBERT

- Avalia√ß√£o com m√©tricas de classifica√ß√£o

- Salvamento do modelo final

O modelo treinado foi publicado no Hugging Face Hub:

`üîó https://huggingface.co/amanda2703/pii-distilbert-hackathon`

